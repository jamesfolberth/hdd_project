\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{caption}
\textheight 240mm
\textwidth  170mm
\oddsidemargin  0mm
\evensidemargin 0mm
\topmargin -20mm
\begin{document}                                       
%_________________________________________________________________
\title{Project Proposal\\
James Folberth, Dale Jennings, Alyson Fox}
\maketitle
%_________________________________________________________________
\section{Introduction}
%_________________________________________________________________
\indent Digital music has changed the face of music collection. Users can have megabyte or a gigabyte worth of music on their personal labtops and can easily download music from the internet. This has lead to the need to invent and test new tools in musical information retrieval. There are many websites and apps, Pandora and iTunes are a few examples, that can create playlists based on similarity to a certain song or genre of music. These sites need to be able to classify certain features of a song and build the playlist from there. 

\indent This goal of this project is to able to classify the genre of a song. This may seem like a simple problem, since we can usually classify a song by ear, but that relies on the user having a vast knowledge of music. If we can automate the process using computers, we could find new and interesting insights that may not be obvious. However, this adds complexity that must be dealt with since we need techniques so that  the computer can ``listen'' to the song and extract features to be able to identify the genre. To do this we take  a song, which is a continuous signal and we sample at certain frequency to transform it into a discrete signal that is based on pitch which created by the sound pressures changes  and define it as a function of time. From the discrete signal we can define  ``features" for each song that many be able to be used to classify the genre. 

\indent For our project we are only classifying within six different genres, classical, electronically, jazz/blues, metal/punk, rock/pop and world. To compute similarities between each song it is imperative that we generate features. Given 729 training tracks, we will construct features for each song and the song that we would like to classify to create a ``difference" to be able to use $k$-means nearest neighbor method. Most features that we used were developed by or used in Elias Pampalk dissertation. \footnote{Computational Models of Music Similarity and their Application in Music Information Retrieval by Elias Pampal } %fix reference
\indent
%_________________________________________________________________
\section{Distance in song-space}
Explain what are the available distance in the space of songs.
Describe your distance and any pre-processing performed before
computing the distance.
%_________________________________________________________________
\section{Dimension reduction}
%_________________________________________________________________
Describe your dimension reduction technique, and justify why it 
is appropriate to use it in this context. You should explain what 
performance is expected.
%_________________________________________________________________
\section{Statistical learning}
%_________________________________________________________________
To validate our classification algorithm, we want to use cross-validation which is a way for us to asses our results if were were to generalize to a new and independent data set. We want to get an idea about how well we are able to predict the genre correctly using a random subset of our training data as the test data since we are have the ground truth. Cross-validation also give us insight in the training phase of our algorithm to determine if we are over fitting. Overfitting the data occurs when the algorithm is predicting the error instead of true trends in the data. This can occur due to many factors, one of which is due to using too many features to describe the model, thus cross-validation can also inform which features or methods provide the best and consistent results. For our purpose we are using a leave-$p$-out-cross-validation method. This consists of leaving out $p$ observations of our training set and using the rest of the observations as our training data. We then repeat with all the different ways to cut the original data and compute the mean and the standard deviation. 

%_________________________________________________________________
\section{Initial results}
Based on our algorithm, if we were to take one song from our training data and used the rest to train the algorithm we have the  following confusion matrix  in Figure {\ref{fig:confMat}}, \begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical& 314 & 3 & 0 & 1 & 4 & 35 \\ \hline 
electronic&0 & 83 & 2 & 1 & 10 & 16 \\ \hline 
jazz&0 & 0 & 23 & 0 & 0 & 1 \\ \hline 
punk& 0 & 1 & 0 & 24 & 7 & 3 \\ \hline 
rock&1 & 20 & 1 & 18 & 80 & 24 \\ \hline 
world&5 & 7 & 0 & 1 & 1 & 43 \\ \hline 
\end{tabular}
\caption{Confusion Matrices}
\label{fig:confMat}
\end{figure}

with the following percent correct for each genre, 

\[  [87.96\%, 74.11\%, 95.83\%, 68.57\%, 55.56\%, 75.44\% ]. \]


To conduct the leave-out-$p$-cross-validation we split our data into five randomly distributed sets, ensuring an even spilt between each genre. When then create a confusion matrix for the test data for each of the 5 subsets and then repeated the process ten times, again randomly distributing the set. We end up with 50 confusion matrices for our cross-validation and we take the mean and standard deviation which are displayed below in Figure {\ref{fig:avgconfMat}}  and Figure {\ref{fig:stdconfMat}} respectively.
\begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical&63 & 0 & 2 & 0 & 1 & 10 \\ \hline 
electronic&0 & 16 & 0 & 0 & 1 & 3 \\ \hline 
jazz&0 & 0 & 1 & 0 & 0 & 0 \\ \hline 
punk&0 & 0 & 0 & 4 & 2 & 0 \\ \hline 
rock&0 & 5 & 2 & 4 & 16 & 7 \\ \hline 
world&1 & 1 & 0 & 0 & 0 & 5 \\ \hline 
\end{tabular}
\caption{Average of the Confusion Matrices} 
\label{fig:avgconfMat}
\end{figure}


\begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical& 1.353604 & 0.635353 & 1.031939 & 0.404061 & 0.856190 & 2.221968 \\ \hline 
electronic& 0.000000 & 1.972153 & 0.350510 & 0.404061 & 0.868731 & 1.595402 \\ \hline 
jazz&0.000000 & 0.000000 & 0.769044 & 0.000000 & 0.000000 & 0.350510 \\ \hline 
punk&0.000000 & 0.567486 & 0.000000 & 1.324803 & 1.629323 & 0.328261 \\ \hline 
rock&0.443087 & 1.982062 & 1.124132 & 1.355262 & 2.006520 & 1.764040 \\ \hline 
world&1.271990 & 0.997139 & 0.609114 & 0.000000 & 0.481918 & 1.925235 \\ \hline 
\end{tabular}
\caption{Standard Deviation of the Confusion Matrices} 
\label{fig:stdconfMat}
\end{figure}

The percent correct for each genre are as follows,
\[ [82.89\%, 80.00\%, 100.00\%, 57.14\%, 48.48\%, 71.43\%] \]
You can see that we are predicting, classical, electronic, jazz  and world with over $70\%$, while punk and jazz we did not do as well. 

%_________________________________________________________________



%_________________________________________________________________
\section{Discussion}

Our algorithm creates a weighted distance between each test observation to all the training observations. another method would be to project our data into the feature space using a singular value decomposition (SVD). We would create a matrix where each feature that we would like to use to describe the data is represented column wise and the song is represented row wise. SVD then decompose our matrix into its singular values and the left and right singular vectors. We can then project our test data down the the feature space which is described by the SVD. We can reduce the dimensionally as well. We choose the $n$ largest singular values and there corresponding left and right singular vectors and use that as an approximation to our feature space. We can create an approximation of the feature space for each of the genre's and project the test song down on to each and then find the $k$-nearest neighbors to be able to determine the predicted genre. 


\end{document}
