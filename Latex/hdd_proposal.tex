\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{caption}
\usepackage{hyperref}
\textheight 240mm
\textwidth  170mm
\oddsidemargin  0mm
\evensidemargin 0mm
\topmargin -20mm
\begin{document}                                       
%_________________________________________________________________
\title{Project Proposal\\
James Folberth, Dale Jennings, Alyson Fox}
\maketitle
%_________________________________________________________________
\section{Introduction}
%_________________________________________________________________
\indent Digital music has changed the face of music collection. Users can have megabyte or a gigabyte worth of music on their personal labtops and can easily download music from the internet. This has lead to the need to invent and test new tools in musical information retrieval. There are many websites and apps, Pandora and iTunes are a few examples, that can create playlists based on similarity to a certain song or genre of music. These sites need to be able to classify certain features of a song and build the playlist from there. 

\indent This goal of this project is to able to classify the genre of a song. This may seem like a simple problem, since we can usually classify a song by ear, but that relies on the user having a vast knowledge of music. If we can automate the process using computers, we could find new and interesting insights that may not be obvious. However, this adds complexity that must be dealt with since we need techniques so that  the computer can ``listen'' to the song and extract features to be able to identify the genre. To do this we take  a song, which is a continuous signal and we sample at certain frequency to transform it into a discrete signal that is based on pitch which created by the sound pressures changes  and define it as a function of time. From the discrete signal we can define  ``features" for each song that many be able to be used to classify the genre. 

\indent For our project we are only classifying within six different genres, classical, electronically, jazz/blues, metal/punk, rock/pop and world. To compute similarities between each song it is imperative that we generate features. Given 729 training tracks, we will construct features for each song and the song that we would like to classify to create a ``difference" to be able to use $k$-means nearest neighbor method. Most features that we used were developed by or used in Elias Pampalk dissertation \cite{pampalk:dissertation} or in Tzanetakis and Cook \cite{tzanetakis:classification}.%\footnote{Computational Models of Music Similarity and their Application in Music Information Retrieval by Elias Pampal } %fix reference
\indent
%_________________________________________________________________
\section{Distance in song-space}


When trying to define a distance in song-space for genre classification, it is not very helpful to try to construct a distance based soley on the raw data.  Instead, feature extraction is applied to the songs in an attempt to represent each song in a manner that is more similar to how the human ear and brain perceives songs.  Having this quantification of musical features allows songs to be classified in a manner more similar to how a human might classify the songs.

\subsection{Preprocessing}

The first step in feature extraction is in preprocessing the song, where the raw pulse code modulation form of the song is transformed into segments of spectral information.  The preprocessing improves the accessibility of the features.  Preprocessing is also the first step in dimensionality reduction.

\subsubsection{Power Spectrum}
The power spectrum of an audio signal is a frequency-domain representation of the signal.  More specifically, the power spectrum is a short time Fourier representation of the signal.  The signal is broken up into many small, possible overlapping segments, each of which then is windowed and passed through a Fourier transform.  Once in the frequency domain, the square magnitude is calculated, to get the power spectrum.

\subsubsection{Mel Frequency Cepstrum}
While the power spectrum of a signal presents some nice information of the signal, the human ear processes audio signals in a different manner.  The Mel Frequency Cepstrum is a way to transform the power spectrum into a scale that is more similar to how the human ear perceives the signal.  Specifically, the Mel-scale is defined to be
$$ m_{Mel} = 1127.01048 \ln(1 + f_{Hz}/700) $$
Instead of looking at a linear spread of frequencies, the Mel-scale frequencies are approximately linear for small (below 500Hz) frequencies and have a logarithmic spread for higher frequencies.  In order to transform the power spectrum into the Mel Frequency Cepstrum, triangular filters are applied to the power spectrum, binning the frequencies appropriately.

The frequency scales are not the only difference with how the ear perceives audio.  The loudness is also perceived in a non-linear way.  The perceived loudness of sounds approximately follows the Decibel (dB) logarithmic scale.  The log of Mel power is calculated to convert to this dB representation.

\subsection{Available Features}

After this preprocessing has been performed, many features become easily extracted.  With these features, a distance can be constructed between songs, with the metric depending on which features are of interest.  Below, some of the possible features are described.  This list is in no way exhaustive, as many other features exist.

\subsubsection{Zero Crossing Rate}

The zero crossing rate (ZCR) is one of the easiest features to extract, as it does not require any preprocessing.  The ZCR measures the average number of times a signal crosses zero, normalized to some unit time.  Generally speaking the ZCR can be used to help classify the noisiness of a signal, although perceived noisiness and perceived ZCR may differ for certain signals (as noted in Pampalk's thesis).  Mathematically, the ZCR is given by
$$ \text{zcr} = \frac{\sum_{i=1}^{N-1} \vert w_{i+1} - w{i} \vert}{2 N f_s} $$
where $\mathbf{w}$ is the raw data, $N$ is the length of the data, and $f_s$ is the sampling frequency.

\subsubsection{Root Mean Square Energy}

The Root Mean Square (RMS) energy is another feature that comes directly from the audio signal.  The RMS energy gives a measure of overall loudness of an audio signal, and is given by
$$ \text{rms} = \sqrt{\frac{1}{N}\sum_{i=1}^N w_i^2} $$


\subsubsection{Noisiness}

As mentioned earlier, the ZCR gives one attempt at classifying noisiness, although it can differ from perceived noisiness.  To improve the measure of noisiness, the dB scale power spectrum of the audio signal can be used.  One measure of this spectrum based noisiness is based off of differences between frequency bins for a given time segment.  Let $\mathbf{P}$ represent the power spectrum of a signal, with rows corresponding to frequency bins, and columns corresponding to time segments.  Then,
$$ \text{noisiness}  = \sum_{t} \sum_{f} \vert P_{f+1,t} - P_{f,t} \vert $$
In practice, frequencies below 800Hz are ignored in this calculation for noisiness.  Low values for this noisiness measure correspond to noisy sound.

\subsubsection{Average Loudness}

When looking at the average loudness of an audio signal, it is useful to refer to the Mel frequencies, as we want to measure perceived average loudness. Let $\mathbf{M}$ represent the matrix of the dB scale mel frequency ceptstrum, then

$$ \text{Avg Loudness} = \frac{1}{N T} \sum_{f,t} M_{f,t} $$

\subsubsection{Percussiveness}

A simple measure for percussiveness comes from the difference between successive time segments of the Mel ceptstrum.
$$ \text{percussiveness} = \frac{1}{N(T-1)} \sum_{f,t} \vert M_{f,t+1} - M_{f,t}  \vert $$

Other measures of percussiveness or rhythmic features can be calculated based on the wavelet transform of the audio signal.  The autocorrelation of a discrete wavelet representation of the audio signal presents periodicity information of the signal, which is related to the percussiveness of the signal.

\subsubsection{Spectral Centroid}

In an attempt to measure the brightness of the signal, the spectral centroid is used.  Heuristically, if a signal has a brighter sound, it is likely to have a lot of energy in the higher frequencies.  Thus, the spectral centroid might be higher.  One limitation, however, is if the signal has a full range of sounds, combining bright high pitch sounds with strong low pitched sounds (e.g. strong drums).
$$ \text{Spectral Centroid} = \frac{\sum_f f*M_{f,t}}{\sum_f M_{f,t}} $$
Note that this spectral centroid measure is for each time segment, and the mean across the entire song is usually used as a comparison feature.

\subsubsection{Fluctuation Patterns}

Fluctuation patterns are useful measures for describing features not captured by spectral similarities.  Fluctuation patterns are descriptors of the loudness modulation within the spectrogram.  Different aspects of these fluctuation parameters are closely related to certain perceptions of audio.  For a description of these relationships, see Pompak's thesis.

The fluctuation patterns are computed by taking the Fourier transform of segments of the spectrogram.  The results are weighted to line up with perceived modulations, and filtered to accentuate specific patterns.  From these fluctuation patterns, certain features can be extracted: maximum fluctuation strength, bass, aggressiveness, low frequency domination, gravity, focus.  Let $\mathbf{FP}$ be the fluctuation pattern matrix, where rows correspond to frequency bands, and columns correspond to modulation frequencies.  Then

\begin{align*}
fp_{max} &= \max_{i,j} \{ FP_{i,j} \} \\
fp_{bass} &= \sum_{i=1}^2 \sum_{j \geq 3} FP_{i,j} \\
fp_{aggr} &= \frac{\sum_{i \geq 2} \sum_{j=1}^4 FP_{i,j} }{ \max_{i,j} \{ FP_{i,j} \}} \\
fp_{DLF} &= \frac{ \sum_{i=1}^3 \sum_j FP_{i,j} }{ \sum_{i\geq 9} \sum_j FP_{i,j} } \\
fp_{grav} &= \frac{ \sum_j j \sum_{i} FP_{i,j} }{ \sum_{i,j} FP_{i,j} } \\
fp_{foc} &= \frac{1}{I J} \sum_{i,j} \frac{FP_{i,j}}{ \max_{i,j} \{ FP_{i,j} \} }
\end{align*}

\subsection{Using Features for Distances}

Once the features of the songs have been collected, a distance between songs can be defined.  For most of the features, a simple Euclidean distance can be used.  However, for more complicated features like the Mel cepstrum, more sophisticated measures need to be used.

\subsubsection{Guassian Modelling}
Work in progress


%_________________________________________________________________
\section{Dimension reduction}
%_________________________________________________________________
Describe your dimension reduction technique, and justify why it 
is appropriate to use it in this context. You should explain what 
performance is expected.
%_________________________________________________________________
\section{Statistical learning}
%_________________________________________________________________
\textbf{Need to talk about machine learning methods here, i.e. kNN, SVM, etc}

To validate our classification algorithm, we want to use cross-validation which is a way for us to asses our results if were were to generalize to a new and independent data set. We want to get an idea about how well we are able to predict the genre correctly using a random subset of our training data as the test data since we are have the ground truth. Cross-validation also give us insight in the training phase of our algorithm to determine if we are over fitting. Overfitting the data occurs when the algorithm is predicting the error instead of true trends in the data. This can occur due to many factors, one of which is due to using too many features to describe the model, thus cross-validation can also inform which features or methods provide the best and consistent results. For our purpose we are using a leave-$p$-out-cross-validation method. This consists of leaving out $p$ observations of our training set and using the rest of the observations as our training data. We then repeat with all the different ways to cut the original data and compute the mean and the standard deviation. 

%_________________________________________________________________
\section{Initial results}
Based on our algorithm, if we were to take one song from our training data and used the rest to train the algorithm we have the  following confusion matrix  in Figure {\ref{fig:confMat}}, \begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical& 314 & 3 & 0 & 1 & 4 & 35 \\ \hline 
electronic&0 & 83 & 2 & 1 & 10 & 16 \\ \hline 
jazz&0 & 0 & 23 & 0 & 0 & 1 \\ \hline 
punk& 0 & 1 & 0 & 24 & 7 & 3 \\ \hline 
rock&1 & 20 & 1 & 18 & 80 & 24 \\ \hline 
world&5 & 7 & 0 & 1 & 1 & 43 \\ \hline 
\end{tabular}
\caption{Confusion Matrices}
\label{fig:confMat}
\end{figure}

with the following percent correct for each genre, 

\[  [87.96\%, 74.11\%, 95.83\%, 68.57\%, 55.56\%, 75.44\% ]. \]


To conduct the leave-out-$p$-cross-validation we split our data into five randomly distributed sets, ensuring an even spilt between each genre. When then create a confusion matrix for the test data for each of the 5 subsets and then repeated the process ten times, again randomly distributing the set. We end up with 50 confusion matrices for our cross-validation and we take the mean and standard deviation which are displayed below in Figure {\ref{fig:avgconfMat}}  and Figure {\ref{fig:stdconfMat}} respectively.
\begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical&63 & 0 & 2 & 0 & 1 & 10 \\ \hline 
electronic&0 & 16 & 0 & 0 & 1 & 3 \\ \hline 
jazz&0 & 0 & 1 & 0 & 0 & 0 \\ \hline 
punk&0 & 0 & 0 & 4 & 2 & 0 \\ \hline 
rock&0 & 5 & 2 & 4 & 16 & 7 \\ \hline 
world&1 & 1 & 0 & 0 & 0 & 5 \\ \hline 
\end{tabular}
\caption{Average of the Confusion Matrices} 
\label{fig:avgconfMat}
\end{figure}


\begin{figure}[h!]
\centering
 \begin{tabular}{l| |l | l | l | l | l | l | }
&classical &electronic& jazz &punk& rock &world \\ \hline \hline
classical& 1.353604 & 0.635353 & 1.031939 & 0.404061 & 0.856190 & 2.221968 \\ \hline 
electronic& 0.000000 & 1.972153 & 0.350510 & 0.404061 & 0.868731 & 1.595402 \\ \hline 
jazz&0.000000 & 0.000000 & 0.769044 & 0.000000 & 0.000000 & 0.350510 \\ \hline 
punk&0.000000 & 0.567486 & 0.000000 & 1.324803 & 1.629323 & 0.328261 \\ \hline 
rock&0.443087 & 1.982062 & 1.124132 & 1.355262 & 2.006520 & 1.764040 \\ \hline 
world&1.271990 & 0.997139 & 0.609114 & 0.000000 & 0.481918 & 1.925235 \\ \hline 
\end{tabular}
\caption{Standard Deviation of the Confusion Matrices} 
\label{fig:stdconfMat}
\end{figure}

The percent correct for each genre are as follows,
\[ [82.89\%, 80.00\%, 100.00\%, 57.14\%, 48.48\%, 71.43\%] \]
You can see that we are predicting, classical, electronic, jazz  and world with over $70\%$, while punk and jazz we did not do as well. 

%_________________________________________________________________



%_________________________________________________________________
\section{Discussion}

Our algorithm creates a weighted distance between each test observation to all the training observations. another method would be to project our data into the feature space using a singular value decomposition (SVD). We would create a matrix where each feature that we would like to use to describe the data is represented column wise and the song is represented row wise. SVD then decompose our matrix into its singular values and the left and right singular vectors. We can then project our test data down the the feature space which is described by the SVD. We can reduce the dimensionally as well. We choose the $n$ largest singular values and there corresponding left and right singular vectors and use that as an approximation to our feature space. We can create an approximation of the feature space for each of the genre's and project the test song down on to each and then find the $k$-nearest neighbors to be able to determine the predicted genre. 


% References
\bibliographystyle{siam}
\bibliography{mir}

\end{document}
